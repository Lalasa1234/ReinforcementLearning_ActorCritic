{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lubsJiqmw8y",
        "outputId": "390d0ef9-2123-4ed4-fa4b-49f3267d7496"
      },
      "id": "6lubsJiqmw8y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WodL4itjFoOx",
        "outputId": "628a34b0-1dee-4b4e-c673-fcafd4bf904e"
      },
      "id": "WodL4itjFoOx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y69fzTdFqJK",
        "outputId": "efb11bd0-a622-4764-cb62-8c7f7ceec2b2"
      },
      "id": "1y69fzTdFqJK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379447 sha256=3574188d4a675f8466fffffdab628323d7ca0e68e6cf5ba41fd4c2092a38799d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QnZAIuDVWL",
        "outputId": "5ac4a7d9-5bf3-4a78-c1bb-21f1b12d7092"
      },
      "id": "L0QnZAIuDVWL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)"
      ],
      "metadata": {
        "id": "pdY480jrDZtz"
      },
      "id": "pdY480jrDZtz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.envs import registry\n",
        "\n",
        "pong_envs = [env_spec.id for env_spec in registry.values() if 'PongNoFrameskip' in env_spec.id]\n",
        "print(pong_envs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HskGpp_VCovh",
        "outputId": "9d434855-3b37-4b19-a5dd-099b1d3b11fc"
      },
      "id": "HskGpp_VCovh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PongNoFrameskip-v0', 'PongNoFrameskip-v4']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "b23be97e-4391-430b-8f66-b18743e0b004"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import gymnasium as gym\n",
        "from gym.wrappers import AtariPreprocessing, FrameStack\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a42f1af9c70>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a – Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        " -------------------------------\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "🔗 Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1A CODE BEGINS\n",
        "# class SeparateActorCritic with separate networks for actor and critic\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden_size=128):\n",
        "        super().__init__()\n",
        "        # Actor network\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        # Critic network\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        probs = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return probs, value"
      ],
      "metadata": {
        "id": "9p-BfNukBL6g"
      },
      "id": "9p-BfNukBL6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim = 4\n",
        "action_dim = 2\n",
        "model = SeparateActorCritic(obs_dim, action_dim)"
      ],
      "metadata": {
        "id": "wTGNI2FPBK8Y"
      },
      "id": "wTGNI2FPBK8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor_optimizer = optim.Adam(model.actor.parameters(), lr=1e-3)\n",
        "critic_optimizer = optim.Adam(model.critic.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "WbSxpCywClVi"
      },
      "id": "WbSxpCywClVi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Actor step ===\n",
        "dummy_input = torch.rand(8, obs_dim)\n",
        "action_probs, _ = model(dummy_input)\n",
        "actions = torch.randint(0, action_dim, (8, 1))\n",
        "log_probs = torch.log(action_probs.gather(1, actions))\n",
        "entropies = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=1, keepdim=True)\n",
        "returns = torch.rand(8, 1)\n",
        "values = model(dummy_input)[1].detach()\n",
        "advantage = returns - values\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropies.mean()\n",
        "\n",
        "actor_optimizer.zero_grad()\n",
        "actor_loss.backward()\n",
        "actor_optimizer.step()"
      ],
      "metadata": {
        "id": "BZilnsukCutr"
      },
      "id": "BZilnsukCutr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Critic step ===\n",
        "_, values = model(dummy_input)\n",
        "critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "critic_optimizer.zero_grad()\n",
        "critic_loss.backward()\n",
        "critic_optimizer.step()\n",
        "\n",
        "# 1A CODE ENDS"
      ],
      "metadata": {
        "id": "zMqz_c3RCyWc"
      },
      "id": "zMqz_c3RCyWc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation for Using Totally Separate Actor and Critic Networks\n",
        "Using totally independent networks, the actor's policy and the critic's value estimation are learned independently from each other, without shared parameters or layers. This is a simple design that has a number of benefits:\n",
        "\n",
        "**No Interference Between Objectives**- The actor and critic optimize different loss functions (policy gradient vs. value regression). Keeping the two separate entirely avoids the risk of the actor's changes negatively influencing the critic's learning processes and vice versa.\n",
        "\n",
        "**Flexible Architecture**- Each network can be designed for their intended functions where actor may take advantage of sharper gradients and modified layers to explore, and critic may need stability with deeper layers to regress\n",
        "\n",
        "**Useful for Heterogeneous Tasks**- In cases when policies and value functions have very different input / output functions of characteristics of the neural networks, a separation is beneficial\n",
        "\n",
        "**Easier to Debug or Analyze**- Separate networks allow for us to debug, analyze, and optimize models independently"
      ],
      "metadata": {
        "id": "R8xm0HTRGolo"
      },
      "id": "R8xm0HTRGolo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When is it preferred in practice?-\n",
        "\n",
        "- For small and medium environments where computational efficiency is not the most important factor\n",
        "- When the settings have high variance returns and the critic may benefit from more stability without needing to be influenced by rapidly changing policies\n",
        "- Basic research settings where interpretability and modifiability are important (e.g., ablation studies)."
      ],
      "metadata": {
        "id": "4gI9QxqJHzH_"
      },
      "id": "4gI9QxqJHzH_"
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b – Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "🔗 More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "id": "a48f882fff11aecc"
      },
      "cell_type": "code",
      "source": [
        "# 1B CODE BEGINS\n",
        "# SharedActorCritic class\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.shared(x)\n",
        "        action_probs = self.actor_head(base)\n",
        "        value = self.critic_head(base)\n",
        "        return action_probs, value"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim = 4\n",
        "action_dim = 2\n",
        "model = SharedActorCritic(obs_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "0JM-q20pIle_"
      },
      "id": "0JM-q20pIle_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input\n",
        "dummy_input = torch.rand(8, obs_dim)\n",
        "action_probs, values = model(dummy_input)"
      ],
      "metadata": {
        "id": "c6FkAS3LIn1J"
      },
      "id": "c6FkAS3LIn1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy actions and returns\n",
        "actions = torch.randint(0, action_dim, (8, 1))\n",
        "log_probs = torch.log(action_probs.gather(1, actions))\n",
        "returns = torch.rand(8, 1)\n",
        "entropies = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "YzYrurfDIqKJ"
      },
      "id": "YzYrurfDIqKJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advantage\n",
        "advantage = returns - values.detach()\n",
        "\n",
        "# Losses\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropies.mean()\n",
        "critic_loss = F.mse_loss(values, returns)\n",
        "total_loss = actor_loss + critic_loss\n",
        "\n",
        "# Backpropagation\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "# 1B CODE ENDS"
      ],
      "metadata": {
        "id": "Dxme36YbIs2f"
      },
      "id": "Dxme36YbIs2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation for Using Shared Actor-Critic Network with Two Heads\n",
        "In this architecture, the actor networks and critic networks share a single base network that branches to specialized heads- the actor head gives actions probabilities or parameters, the critic head gives a value estimation for the state.\n",
        "\n",
        "**Shared Representation Learning**- The base network has the ability to learn features that are useful for both policy learning and value learning. This can be especially important in environments where the state features are complex or high-dimensional (e.g., images, robotics).\n",
        "\n",
        "**Efficiency in Parameters**- This reduces the total number of network parameters compared to using separate networks. This efficiency is critical when running agents on resource-constrained systems or in large environments (e.g., Atari, MuJoCo).\n",
        "\n",
        "**Improving Generalization**- The shared learning of features encourages a more consistent and shared understanding of the environment which can improve the stability and convergence of both the policy and value estimation. This is important in environments with either dense or continuous observations or in environments with observations that are based on image data.\n",
        "\n",
        "**Simplified Training Pipeline**- Often a single optimizer and a single forward pass through the original base network is adequate. If integrated effectively this code reuse and implementation of one forward pass is significantly faster and more efficient especially when leveraging libraries and libraries for multi-task RL settings."
      ],
      "metadata": {
        "id": "u9KtPA-fZG5j"
      },
      "id": "u9KtPA-fZG5j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When is it preferred in practice?-\n",
        "\n",
        "- This architecture is preferred with high-dimensional or complex inputs (e.g., image based observations from Atari or robotics sensors) and if computational efficiency is important (ex. real-time agents).\n",
        "- When policy learning and value learning are learned with relatively shared features (i.e., have a similar reward structure) generally prefers this architecture.\n",
        "- This is also useful during prototyping, or when leveraging modular libraries (e.g., TensorFlow Agents, Stable Baselines, RLlib)."
      ],
      "metadata": {
        "id": "-wlz4L7WakDM"
      },
      "id": "-wlz4L7WakDM"
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "🔗 Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a"
      },
      "cell_type": "code",
      "source": [
        "class SharedActorCriticAuto(nn.Module):\n",
        "    def __init__(self, obs_dim, action_space, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.is_discrete = isinstance(action_space, gym.spaces.Discrete)\n",
        "        self.is_box = isinstance(action_space, gym.spaces.Box)\n",
        "\n",
        "        if self.is_discrete:\n",
        "            self.actor = nn.Sequential(\n",
        "                nn.Linear(hidden_size, action_space.n),\n",
        "                nn.Softmax(dim=-1)\n",
        "            )\n",
        "        elif self.is_box:\n",
        "            action_dim = action_space.shape[0]\n",
        "            self.actor_mean = nn.Linear(hidden_size, action_dim)\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unsupported action space\")\n",
        "\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.shared(x)\n",
        "        value = self.critic(base)\n",
        "        if self.is_discrete:\n",
        "            action_probs = self.actor(base)\n",
        "            return action_probs, value\n",
        "        else:\n",
        "            mean = self.actor_mean(base)\n",
        "            log_std = self.actor_log_std.expand_as(mean)\n",
        "            return (mean, log_std), value"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_obs(env, obs):\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        one_hot = np.zeros(env.observation_space.n, dtype=np.float32)\n",
        "        one_hot[obs] = 1.0\n",
        "        return torch.tensor(one_hot).unsqueeze(0)\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        obs = np.array(obs, dtype=np.float32)\n",
        "        if obs.ndim > 1:\n",
        "            obs = obs.flatten()  # For images or frame stacks\n",
        "        return torch.tensor(obs).unsqueeze(0)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported observation space\")"
      ],
      "metadata": {
        "id": "rnqvSFVGFvD5"
      },
      "id": "rnqvSFVGFvD5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_shared_network(env):\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        obs_dim = env.observation_space.n\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        obs_dim = int(np.prod(env.observation_space.shape))\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported observation space\")\n",
        "\n",
        "    return SharedActorCriticAuto(obs_dim, env.action_space)"
      ],
      "metadata": {
        "id": "LDv74ePSF1qy"
      },
      "id": "LDv74ePSF1qy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_ids = [\"CliffWalking-v0\", \"LunarLander-v3\", \"HalfCheetah-v5\"]\n",
        "\n",
        "for env_id in env_ids:\n",
        "    print(f\"\\n=== Testing {env_id} ===\")\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    model = create_shared_network(env)\n",
        "\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs = obs[0]\n",
        "\n",
        "    processed_obs = preprocess_obs(env, obs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(processed_obs)\n",
        "\n",
        "    print(f\"Output type: {type(output)}\")\n",
        "    if isinstance(output, tuple):\n",
        "        print(f\"Actor output shape: {output[0] if isinstance(output[0], torch.Tensor) else 'Gaussian tuple'}\")\n",
        "        print(f\"Critic output shape: {output[1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaycRmxlGDGq",
        "outputId": "0560963f-1f52-44c2-a3a9-f0471e322ed0"
      },
      "id": "EaycRmxlGDGq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing CliffWalking-v0 ===\n",
            "Output type: <class 'tuple'>\n",
            "Actor output shape: tensor([[0.2492, 0.2529, 0.2381, 0.2597]])\n",
            "Critic output shape: torch.Size([1, 1])\n",
            "\n",
            "=== Testing LunarLander-v3 ===\n",
            "Output type: <class 'tuple'>\n",
            "Actor output shape: tensor([[0.2595, 0.2355, 0.2647, 0.2402]])\n",
            "Critic output shape: torch.Size([1, 1])\n",
            "\n",
            "=== Testing HalfCheetah-v5 ===\n",
            "Output type: <class 'tuple'>\n",
            "Actor output shape: Gaussian tuple\n",
            "Critic output shape: torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'PongNoFrameskip-v4'\n",
        "print(f\"\\n=== Testing {env_id} ===\")\n",
        "\n",
        "env = gym.make(env_id)\n",
        "\n",
        "model = create_shared_network(env)\n",
        "\n",
        "obs = env.reset()\n",
        "if isinstance(obs, tuple):\n",
        "    obs = obs[0]\n",
        "\n",
        "processed_obs = preprocess_obs(env, obs)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(processed_obs)\n",
        "\n",
        "print(f\"Output type: {type(output)}\")\n",
        "if isinstance(output, tuple):\n",
        "    print(f\"Actor output shape: {output[0] if isinstance(output[0], torch.Tensor) else 'Gaussian tuple'}\")\n",
        "    print(f\"Critic output shape: {output[1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25iwPDdTbsT8",
        "outputId": "0a03f060-17d3-4578-d9ee-897c960ec355"
      },
      "id": "25iwPDdTbsT8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing PongNoFrameskip-v4 ===\n",
            "Output type: <class 'tuple'>\n",
            "Actor output shape: tensor([[6.2994e-01, 1.3675e-23, 3.7006e-01, 1.4152e-20, 3.7782e-24, 7.9731e-28]])\n",
            "Critic output shape: torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation for Using Auto-Adaptive Network Setup\n",
        "The Auto-Adaptive Network Setup entails constructing neural network architectures which can flexibly adapt to any observation and action spaces of the environment. Instead of hardcoding input-output dimensions, the network refers to the environment and adapts itself.\n",
        "\n",
        "**Generalization Across Environments**- Reinforcement learning agents are now reusable in multiple environments (e.g., Atari, MuJoCo, GridWorld) without having to rewrite the architecture of the model.Facilitates rendering modularity and cleaner programming.\n",
        "\n",
        "**Faster Prototyping and Testing**- Adapts automatically to the environment structure, which allows researchers to examine new tasks or environments quickly and reliably.Greatly useful in automated pipelines or for hyperparameter sweeps.\n",
        "\n",
        "**Robustness Against Variation**\n",
        "\n",
        "*Works against different types of observations*:\n",
        " observations → one-hot encoded. Box (vector/image) observations → flattened or normalized.\n",
        "\n",
        "*Handles both discrete and continuous action*: Discrete actions → probabilities. Continuous actions → gaussian parameters.\n",
        "\n",
        "**Essential for Multi-Task or Meta-RL:**- Where an agent is trained in multiple tasks with different spaces.Facilitates scaling across tasks with only minimal code changes."
      ],
      "metadata": {
        "id": "TX7tBzHnoBS2"
      },
      "id": "TX7tBzHnoBS2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When is it preferred in practice?-\n",
        "\n",
        "- Within frameworks or libraries that support many environments (e.g., OpenAI Gym, Meta-RL).\n",
        "- Within course projects or research within benchmarking multiple tasks.\n",
        "- Within production or robotics in evolving environments, thus needing dynamic reconfiguration\n",
        "- Designing generalizable RL agents not hardwired to be in a specific environment."
      ],
      "metadata": {
        "id": "0jEUJHOnyoYo"
      },
      "id": "0jEUJHOnyoYo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Observation Normalization"
      ],
      "metadata": {
        "id": "DXBQSAzwQSuO"
      },
      "id": "DXBQSAzwQSuO"
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29"
      },
      "cell_type": "code",
      "source": [
        "def normalize_observation(obs, env):\n",
        "    space = env.observation_space\n",
        "\n",
        "    if isinstance(space, gym.spaces.Box):\n",
        "        obs = np.array(obs, dtype=np.float32)\n",
        "\n",
        "        # Normalize image observations\n",
        "        if np.issubdtype(obs.dtype, np.integer) and obs.max() > 1:\n",
        "            return obs / 255.0\n",
        "        if hasattr(space, \"low\") and hasattr(space, \"high\"):\n",
        "            low = space.low\n",
        "            high = space.high\n",
        "            # Prevent divide-by-zero\n",
        "            scale = np.where(high - low == 0, 1.0, high - low)\n",
        "            return (obs - low) / scale\n",
        "\n",
        "    # For Discrete states\n",
        "    return obs"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "envs = [\"LunarLander-v3\"]\n",
        "env = gym.make(env_id)\n",
        "obs, _ = env.reset() if isinstance(env.reset(), tuple) else (env.reset(), {})\n",
        "norm_obs = normalize_observation(obs, env)\n",
        "\n",
        "print(f\"\\n=== {env_id} ===\")\n",
        "print(f\"Original dtype: {np.array(obs).dtype}, shape: {np.array(obs).shape}\")\n",
        "print(f\"Normalized dtype: {norm_obs.dtype}, shape: {norm_obs.shape}\")\n",
        "print(f\"Min: {norm_obs.min():.4f}, Max: {norm_obs.max():.4f}\")"
      ],
      "metadata": {
        "id": "E31hilr3G2nt",
        "outputId": "8a74ccc1-439d-4082-c1af-7d5e5ff22553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E31hilr3G2nt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LunarLander-v3 ===\n",
            "Original dtype: float32, shape: (8,)\n",
            "Normalized dtype: float32, shape: (8,)\n",
            "Min: 0.0000, Max: 0.7819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "envs = [\"PongNoFrameskip-v4\"]\n",
        "env = gym.make(env_id)\n",
        "obs, _ = env.reset() if isinstance(env.reset(), tuple) else (env.reset(), {})\n",
        "norm_obs = normalize_observation(obs, env)\n",
        "\n",
        "print(f\"\\n=== {env_id} ===\")\n",
        "print(f\"Original dtype: {np.array(obs).dtype}, shape: {np.array(obs).shape}\")\n",
        "print(f\"Normalized dtype: {norm_obs.dtype}, shape: {norm_obs.shape}\")\n",
        "print(f\"Min: {norm_obs.min():.4f}, Max: {norm_obs.max():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvGLXcQRb4Hw",
        "outputId": "33dd3211-6c3d-4e7d-8341-786fc1b7ec2e"
      },
      "id": "EvGLXcQRb4Hw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PongNoFrameskip-v4 ===\n",
            "Original dtype: uint8, shape: (210, 160, 3)\n",
            "Normalized dtype: float64, shape: (210, 160, 3)\n",
            "Min: 0.0000, Max: 0.8941\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Motivation for Observation Normalization\n",
        "Creating an Observation Normalization Function with an Auto-Adaptive Network Architecture is a good approach in reinforcement learning (RL) when the observation space can vary in scale and type and environment variety ranges in complexity.\n",
        "\n",
        "**Make Learning More Stable**- Observations across different environments, even when they are similar in type can have annotation values (features) that have massively different ranges of values.\n",
        "\n",
        "Examples include (but are not limited to):\n",
        "- LunarLander (continuous values in varying ranges; position, velocity, angle, etc.).\n",
        "- Pong (pixels assigned intensity values in the range of 0-255).\n",
        "- CliffWalking (discrete states represented with integers).\n",
        "\n",
        "Learning how components of a model interact together quickly becomes complex when assigned observations and attribution values vary across the scale without a normalization mechanism.Without normalizing observations, Neural networks will struggle to learn, especially when the scales of input features are not consistent.Gradient updates can become unstable leading to a lack of convergence entirely or poor convergence. Normalizing observations provides a mapping for input values to a standard range of values, typically [0, 1] or [-1, 1], which helps normalize observation features into a more learning stable structure.\n",
        "\n",
        "**Generalizing Performance Across Auto-Adaptive Networks**- Our create_shared_network(env) adapts models based on the construction details of arbitrary environments. To generalize our controller across varied environments -- it must abstract away observation scale differences and with normalization this ensures a consistent observation behavior, and as such a more reusable and environment agnostic shared network.\n",
        "\n",
        "**Avoid Implicit Bias**- Without normalized input features across a model, observed features with larger values will dominate the learning (similar to velocities vs. contact booleans of LunarLander). This will promote an uneven model (implicit bias) on the dimensional state. With normalization we are creating a potential of equal footing for all input features and in a sense letting the agent decide what is indeed important."
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When is it preferred in practice?-\n",
        "\n",
        "- For Continuous control (e.g., HalfCheetah, LunarLander) where Large variation across feature scales will affect learning.\n",
        "- For Image RL (e.g., Pong, Breakout) while Normalizing pixel intensity values into [0,1].\n",
        "- Across multiple environments (e.g., meta-RL, AutoRL setups) for a consistent behavior across contexts."
      ],
      "metadata": {
        "id": "-eK_zkU1WQ5A"
      },
      "id": "-eK_zkU1WQ5A"
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates."
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify it’s applied.\n",
        "\n",
        "🔗 PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html"
      ],
      "metadata": {
        "id": "sNx1Dk6-QiEd"
      },
      "id": "sNx1Dk6-QiEd"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1126bb7-ea5a-42bd-d932-10aa8cc6b695"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "obs_dim = 10\n",
        "action_dim = 4\n",
        "model = SharedActorCriticAuto(obs_dim, gym.spaces.Discrete(action_dim))\n",
        "actor_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "critic_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "dummy_input = torch.rand(8, obs_dim)\n",
        "action_probs, _ = model(dummy_input)\n",
        "actions = torch.randint(0, action_dim, (8, 1))\n",
        "log_probs = torch.log(action_probs.gather(1, actions))\n",
        "entropies = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=1, keepdim=True)\n",
        "returns = torch.rand(8, 1)\n",
        "values = model(dummy_input)[1].detach()\n",
        "advantage = returns - values\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropies.mean()\n",
        "\n",
        "actor_optimizer.zero_grad()\n",
        "actor_loss.backward()\n",
        "total_norm_before = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e10)\n",
        "print(f\"[Actor] Gradient norm before clipping: {total_norm_before:.4f}\")\n",
        "\n",
        "total_norm_after = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "print(f\"[Actor] Gradient norm after clipping:  {total_norm_after:.4f}\")\n",
        "\n",
        "actor_optimizer.step()\n",
        "_, values = model(dummy_input)\n",
        "critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "critic_optimizer.zero_grad()\n",
        "critic_loss.backward()\n",
        "total_norm_before = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e10)\n",
        "print(f\"[Critic] Gradient norm before clipping: {total_norm_before:.4f}\")\n",
        "\n",
        "total_norm_after = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "print(f\"[Critic] Gradient norm after clipping:  {total_norm_after:.4f}\")\n",
        "\n",
        "critic_optimizer.step()"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Actor] Gradient norm before clipping: 0.3768\n",
            "[Actor] Gradient norm after clipping:  0.3768\n",
            "[Critic] Gradient norm before clipping: 2.6152\n",
            "[Critic] Gradient norm after clipping:  2.6152\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation for Gradient Clipping\n",
        "**Prevent Exploding Gradients**- When large gradients cause problematic backpropagation, they can create unstable updates to model parameters.This can cause:-Nan losses,-blown up model weights,-dramatic divergence in training.Clipping makes sure that gradient values cap out at a maximum value to keep it in a range that is safe, and to promote more controlled and stable learning.\n",
        "\n",
        "**Stabilize Actor-Critic Learning**- In RL (especially actor-critic algorithms) there is a lot of dependence on the actor and critic.If the gradients in one of the two networks explodes, this can destabilize the other. By clipping the gradients we can avoid feedback loops of instability between the actor and the critic.\n",
        "\n",
        "** Better Optimization Dynamics**- Large gradient norms can cause the optimizer to overshoot the local minimum.Clipping works as a damping effect, which is particularly useful when using adaptive optimizers such as Adam."
      ],
      "metadata": {
        "id": "jT6bbjt8YxAA"
      },
      "id": "jT6bbjt8YxAA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When is it preferred in practice?-\n",
        "\n",
        "- In PPO, A2C, DDPG To increase stability by learning from noisy/unstable returns.\n",
        "- In Recurrent Neural Networks (LSTMs/GRUs) Because RNNs are highly prone to exploding gradients.\n",
        "- In Large or Deep Networks as The deeper you go the more at risk you are for exploding gradients\n",
        "- For Sparse or High Variance Rewards Because it is not uncommon for gradient magnitudes to spike without clear reason."
      ],
      "metadata": {
        "id": "SX45iQMtZcIu"
      },
      "id": "SX45iQMtZcIu"
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "\n",
        "| Lalasa  | Task 1 | 100  |\n",
        "\n",
        "| Lalasa  | Task 2 |  100 |\n",
        "\n",
        "| Lalasa   | Task 3 |  100 |\n",
        "\n",
        "| Lalasa  | Task 4 | 100  |\n",
        "\n",
        "| Both  | **Total** | 100  |\n",
        "\n",
        "Part 2: Kanisha - 100%"
      ],
      "id": "f4cff31e6c6e7e4a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}